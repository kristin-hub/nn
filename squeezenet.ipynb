{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "squeezenet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kristin-hub/nn/blob/master/22squeezenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-Wb-GxXQxSA",
        "colab_type": "text"
      },
      "source": [
        "**Подключаем датасет с гугл диска**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btg-Fwix6Q8f",
        "colab_type": "code",
        "outputId": "be0a1f32-1e1a-46fd-b5be-201666001f1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Most Recent One\n",
        "!pip install git+https://github.com/rcmalli/keras-squeezenet.git\n",
        "# Release Version\n",
        "!pip install keras_squeezenet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/rcmalli/keras-squeezenet.git\n",
            "  Cloning https://github.com/rcmalli/keras-squeezenet.git to /tmp/pip-req-build-uzisazaj\n",
            "  Running command git clone -q https://github.com/rcmalli/keras-squeezenet.git /tmp/pip-req-build-uzisazaj\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras-squeezenet==0.4) (1.17.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras-squeezenet==0.4) (1.3.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-squeezenet==0.4) (2.8.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from keras-squeezenet==0.4) (1.15.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-squeezenet==0.4) (2.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras-squeezenet==0.4) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras-squeezenet==0.4) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras-squeezenet==0.4) (1.0.8)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras-squeezenet==0.4) (1.15.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras-squeezenet==0.4) (3.1.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras-squeezenet==0.4) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras-squeezenet==0.4) (0.1.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras-squeezenet==0.4) (0.8.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras-squeezenet==0.4) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras-squeezenet==0.4) (3.10.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras-squeezenet==0.4) (0.33.6)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras-squeezenet==0.4) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras-squeezenet==0.4) (1.11.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras-squeezenet==0.4) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras-squeezenet==0.4) (1.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras-squeezenet==0.4) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->keras-squeezenet==0.4) (41.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->keras-squeezenet==0.4) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->keras-squeezenet==0.4) (3.1.1)\n",
            "Building wheels for collected packages: keras-squeezenet\n",
            "  Building wheel for keras-squeezenet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-squeezenet: filename=keras_squeezenet-0.4-cp36-none-any.whl size=4424 sha256=d5d46d7c21d91ef4681e7bbca17075014dd09f038ae206d4c0d541ea36f87cdb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g62h2fe4/wheels/15/75/ed/45fffbc76d921a5be07af533b774b35bbf40551334c51af91f\n",
            "Successfully built keras-squeezenet\n",
            "Installing collected packages: keras-squeezenet\n",
            "Successfully installed keras-squeezenet-0.4\n",
            "Requirement already satisfied: keras_squeezenet in /usr/local/lib/python3.6/dist-packages (0.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras_squeezenet) (1.3.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras_squeezenet) (2.2.5)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras_squeezenet) (1.17.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras_squeezenet) (2.8.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from keras_squeezenet) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras_squeezenet) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras_squeezenet) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras_squeezenet) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->keras_squeezenet) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_squeezenet) (0.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_squeezenet) (1.15.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_squeezenet) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_squeezenet) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_squeezenet) (3.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_squeezenet) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_squeezenet) (1.11.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_squeezenet) (0.33.6)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_squeezenet) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_squeezenet) (0.1.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_squeezenet) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_squeezenet) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow->keras_squeezenet) (41.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->keras_squeezenet) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->keras_squeezenet) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ0tjFVrzzfm",
        "colab_type": "code",
        "outputId": "fd03fcff-9be7-4ca3-e4f2-f7c9d172db56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cF7tAvgxz8Ts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir, rename\n",
        "from os.path import isfile, join\n",
        "\n",
        "source_dir = 'drive/My Drive/data'\n",
        "train_dir = 'drive/My Drive/data/train'\n",
        "validation_dir = 'drive/My Drive/data/validation'\n",
        "test_dir = 'drive/My Drive/data/test'\n",
        "\n",
        "fake_dir = 'fake'\n",
        "orig_dir = 'orig'\n",
        "\n",
        "images = [f for f in listdir(source_dir) if isfile(join(source_dir, f))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVEXD5l90HjE",
        "colab_type": "code",
        "outputId": "4aac6098-3ee0-4398-b2d7-424ff392ee33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "\n",
        "np.random.seed(0)\n",
        "tf.set_random_seed(0)\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "set_session(tf.Session(config=config))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O5QN1TiTG5W",
        "colab_type": "text"
      },
      "source": [
        "**Проверяем включена ли GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdcgcrJT0OcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E1aUmrE0fjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_dir = 'drive/My Drive/data/train'\n",
        "test_data_dir = 'drive/My Drive/data/test'\n",
        "validation_data_dir = 'drive/My Drive/data/validation'\n",
        "\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "\n",
        "batch_size = 16\n",
        "epochs = 150\n",
        "\n",
        "train_first_class = 4112\n",
        "train_second_class = 4112\n",
        "\n",
        "val_first_class = 1120\n",
        "val_second_class = 1120\n",
        "\n",
        "nb_train_samples = train_first_class + train_second_class\n",
        "nb_validation_samples = val_first_class + val_second_class\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJu2VcsbHPK0",
        "colab_type": "code",
        "outputId": "df749b28-0e25-4b1f-bd8b-05d497bf034f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!python -c \"import keras; print(keras.__version__)\"\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "2.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVKCB5X4OuBV",
        "colab_type": "code",
        "outputId": "fa4f0433-7172-4fae-d4e6-ec870a6d6f7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "!pip install q keras==2.1.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting q\n",
            "  Downloading https://files.pythonhosted.org/packages/53/bc/51619d89e0bd855567e7652fa16d06f1ed36a85f108a7fe71f6629bf719d/q-2.6-py2.py3-none-any.whl\n",
            "Collecting keras==2.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/49/decea3cbcfc08bd6b2105c5793d958ea159d07f2b43e45a00afe2b35633d/Keras-2.1.1-py2.py3-none-any.whl (302kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.1) (1.3.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.1) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.1) (1.17.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.1) (1.12.0)\n",
            "\u001b[31mERROR: textgenrnn 1.4.1 has requirement keras>=2.1.5, but you'll have keras 2.1.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: q, keras\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "Successfully installed keras-2.1.1 q-2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QiONDpoD63O",
        "colab_type": "text"
      },
      "source": [
        "**Прогоняем наш датасет через сверточные слои предобученной сети Xception, чтобы получить признаки и сохраняем их**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-0UpJkR1X6n",
        "colab_type": "code",
        "outputId": "aa987cc5-eb2d-40cf-d659-7934bae2a2cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from keras.layers import Input\n",
        "#from keras.applications.inception_v3 import InceptionV3\n",
        "#from keras.applications.mobilenet_v2 import MobileNetV2 \n",
        "#from keras.applications.mobilenetv3 import MobileNetV3\n",
        "\n",
        "from keras_squeezenet import SqueezeNet\n",
        "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
        "from keras.preprocessing import image\n",
        "\n",
        "\n",
        "\n",
        "input_tensor = Input(shape=(img_height,img_width,3))\n",
        "\n",
        "base_model = SqueezeNet(weights='imagenet',\n",
        "                          include_top=False,\n",
        "                          input_shape=(img_width, img_height, 3),\n",
        "                          pooling='avg')\n",
        "'''\n",
        "inc_model=InceptionV3(include_top=False,\n",
        "                      weights='imagenet',\n",
        "                      input_shape=((3, 150, 150)))'''\n",
        "\n",
        "\n",
        "data_generator = image.ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = data_generator.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=None,\n",
        "    shuffle=False)\n",
        "\n",
        "bottleneck_features_train = base_model.predict_generator(\n",
        "        train_generator, \n",
        "        nb_train_samples // batch_size)\n",
        "\n",
        "print('Prediction of the training set finished.')\n",
        "\n",
        "np.save(open('bottleneck_features_train.npy', 'wb'),\n",
        "            bottleneck_features_train)\n",
        "\n",
        "validation_generator = data_generator.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=None,\n",
        "    shuffle=False)\n",
        "\n",
        "bottleneck_features_validation = base_model.predict_generator(\n",
        "        validation_generator, \n",
        "        nb_validation_samples // batch_size)\n",
        "\n",
        "print('Prediction of the validation set finished.')\n",
        "\n",
        "np.save(open('bottleneck_features_validation.npy', 'wb'),\n",
        "            bottleneck_features_validation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8224 images belonging to 2 classes.\n",
            "Prediction of the training set finished.\n",
            "Found 2240 images belonging to 2 classes.\n",
            "Prediction of the validation set finished.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-AdPmFKFKN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuCox5Ev1sWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = np.load(open('bottleneck_features_train.npy', 'rb'))\n",
        "train_labels = np.array([0] * train_first_class + [1] * train_second_class)\n",
        "\n",
        "validation_data = np.load(open('bottleneck_features_validation.npy', 'rb'))\n",
        "validation_labels = np.array([0] * (val_first_class) + [1] * (val_second_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NivdVMbkN5yz",
        "colab_type": "text"
      },
      "source": [
        "**Создадим полносвязную сеть и обучим её на полученных признаках**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0bhS3d3_whU",
        "colab_type": "code",
        "outputId": "13dca54b-4fae-43cb-c5dc-ebbf4e4fa520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dropout, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(512, activation='relu', input_shape=base_model.output_shape[1:]))\n",
        "model.add(Dropout(0.5))\n",
        "    \n",
        "model.add(Dense(256, activation='relu', input_shape=base_model.output_shape[1:]))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(128, activation='relu', input_shape=base_model.output_shape[1:]))\n",
        "model.add(Dropout(0.5))    \n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))   \n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=SGD(lr=0.005),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='top-weights.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "history = model.fit(train_data,\n",
        "                    train_labels,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    callbacks=[checkpointer],\n",
        "                    validation_data=(validation_data, validation_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8224 samples, validate on 2240 samples\n",
            "Epoch 1/150\n",
            "8176/8224 [============================>.] - ETA: 0s - loss: 0.8159 - acc: 0.5053Epoch 00001: val_loss improved from inf to 0.69340, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 6s 692us/step - loss: 0.8150 - acc: 0.5058 - val_loss: 0.6934 - val_acc: 0.5000\n",
            "Epoch 2/150\n",
            "8208/8224 [============================>.] - ETA: 0s - loss: 0.6995 - acc: 0.5039Epoch 00002: val_loss improved from 0.69340 to 0.69336, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 3s 380us/step - loss: 0.6996 - acc: 0.5034 - val_loss: 0.6934 - val_acc: 0.4701\n",
            "Epoch 3/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6942 - acc: 0.5004Epoch 00003: val_loss improved from 0.69336 to 0.69314, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 3s 394us/step - loss: 0.6942 - acc: 0.4995 - val_loss: 0.6931 - val_acc: 0.4996\n",
            "Epoch 4/150\n",
            "8208/8224 [============================>.] - ETA: 0s - loss: 0.6935 - acc: 0.4981Epoch 00004: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 435us/step - loss: 0.6935 - acc: 0.4979 - val_loss: 0.6933 - val_acc: 0.5058\n",
            "Epoch 5/150\n",
            "8064/8224 [============================>.] - ETA: 0s - loss: 0.6935 - acc: 0.4856Epoch 00005: val_loss improved from 0.69314 to 0.69313, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 3s 423us/step - loss: 0.6935 - acc: 0.4863 - val_loss: 0.6931 - val_acc: 0.4996\n",
            "Epoch 6/150\n",
            "8080/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.4981Epoch 00006: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 423us/step - loss: 0.6932 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.4996\n",
            "Epoch 7/150\n",
            "8176/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5024Epoch 00007: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 407us/step - loss: 0.6932 - acc: 0.5023 - val_loss: 0.6932 - val_acc: 0.4839\n",
            "Epoch 8/150\n",
            "8128/8224 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.4996Epoch 00008: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 433us/step - loss: 0.6933 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.4978\n",
            "Epoch 9/150\n",
            "8080/8224 [============================>.] - ETA: 0s - loss: 0.6935 - acc: 0.4931Epoch 00009: val_loss improved from 0.69313 to 0.69311, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 3s 400us/step - loss: 0.6935 - acc: 0.4937 - val_loss: 0.6931 - val_acc: 0.5085\n",
            "Epoch 10/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6934 - acc: 0.4977Epoch 00010: val_loss improved from 0.69311 to 0.69303, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 3s 423us/step - loss: 0.6934 - acc: 0.4981 - val_loss: 0.6930 - val_acc: 0.5000\n",
            "Epoch 11/150\n",
            "8160/8224 [============================>.] - ETA: 0s - loss: 0.6935 - acc: 0.4963Epoch 00011: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 442us/step - loss: 0.6935 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.5022\n",
            "Epoch 12/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.5048Epoch 00012: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 438us/step - loss: 0.6933 - acc: 0.5049 - val_loss: 0.6931 - val_acc: 0.5009\n",
            "Epoch 13/150\n",
            "8128/8224 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.5012Epoch 00013: val_loss improved from 0.69303 to 0.69301, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 3s 407us/step - loss: 0.6933 - acc: 0.5013 - val_loss: 0.6930 - val_acc: 0.5219\n",
            "Epoch 14/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5105Epoch 00014: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 425us/step - loss: 0.6931 - acc: 0.5101 - val_loss: 0.6931 - val_acc: 0.5013\n",
            "Epoch 15/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5051Epoch 00015: val_loss improved from 0.69301 to 0.69281, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 3s 396us/step - loss: 0.6932 - acc: 0.5054 - val_loss: 0.6928 - val_acc: 0.4996\n",
            "Epoch 16/150\n",
            "8160/8224 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.4988Epoch 00016: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 430us/step - loss: 0.6933 - acc: 0.4988 - val_loss: 0.6931 - val_acc: 0.5022\n",
            "Epoch 17/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5039Epoch 00017: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 431us/step - loss: 0.6932 - acc: 0.5039 - val_loss: 0.6931 - val_acc: 0.5004\n",
            "Epoch 18/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6934 - acc: 0.5012Epoch 00018: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 393us/step - loss: 0.6934 - acc: 0.5012 - val_loss: 0.6931 - val_acc: 0.5022\n",
            "Epoch 19/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5026Epoch 00019: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 403us/step - loss: 0.6931 - acc: 0.5028 - val_loss: 0.6930 - val_acc: 0.5013\n",
            "Epoch 20/150\n",
            "8096/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5135Epoch 00020: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 398us/step - loss: 0.6931 - acc: 0.5125 - val_loss: 0.6931 - val_acc: 0.5027\n",
            "Epoch 21/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5098Epoch 00021: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 397us/step - loss: 0.6932 - acc: 0.5098 - val_loss: 0.6931 - val_acc: 0.5036\n",
            "Epoch 22/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5021Epoch 00022: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 397us/step - loss: 0.6932 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.5027\n",
            "Epoch 23/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6929 - acc: 0.5078Epoch 00023: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 405us/step - loss: 0.6927 - acc: 0.5092 - val_loss: 0.6933 - val_acc: 0.5000\n",
            "Epoch 24/150\n",
            "8080/8224 [============================>.] - ETA: 0s - loss: 0.6935 - acc: 0.5084Epoch 00024: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 402us/step - loss: 0.6935 - acc: 0.5086 - val_loss: 0.6930 - val_acc: 0.5094\n",
            "Epoch 25/150\n",
            "8176/8224 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.5048Epoch 00025: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 376us/step - loss: 0.6933 - acc: 0.5051 - val_loss: 0.6931 - val_acc: 0.5165\n",
            "Epoch 26/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5005Epoch 00026: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 412us/step - loss: 0.6931 - acc: 0.4999 - val_loss: 0.6930 - val_acc: 0.5232\n",
            "Epoch 27/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.5016Epoch 00027: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 423us/step - loss: 0.6933 - acc: 0.5016 - val_loss: 0.6929 - val_acc: 0.5210\n",
            "Epoch 28/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5120Epoch 00028: val_loss improved from 0.69281 to 0.69276, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 3s 408us/step - loss: 0.6932 - acc: 0.5125 - val_loss: 0.6928 - val_acc: 0.5308\n",
            "Epoch 29/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5060Epoch 00029: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 413us/step - loss: 0.6932 - acc: 0.5060 - val_loss: 0.6929 - val_acc: 0.5147\n",
            "Epoch 30/150\n",
            "8160/8224 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.5029Epoch 00030: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 389us/step - loss: 0.6930 - acc: 0.5030 - val_loss: 0.6928 - val_acc: 0.5281\n",
            "Epoch 31/150\n",
            "8096/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5026Epoch 00031: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 415us/step - loss: 0.6932 - acc: 0.5040 - val_loss: 0.6930 - val_acc: 0.4996\n",
            "Epoch 32/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6934 - acc: 0.5062Epoch 00032: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 401us/step - loss: 0.6933 - acc: 0.5071 - val_loss: 0.6929 - val_acc: 0.5183\n",
            "Epoch 33/150\n",
            "8096/8224 [============================>.] - ETA: 0s - loss: 0.6934 - acc: 0.5017Epoch 00033: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 410us/step - loss: 0.6934 - acc: 0.5009 - val_loss: 0.6930 - val_acc: 0.5049\n",
            "Epoch 34/150\n",
            "8128/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5091Epoch 00034: val_loss improved from 0.69276 to 0.69257, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 3s 425us/step - loss: 0.6931 - acc: 0.5095 - val_loss: 0.6926 - val_acc: 0.5371\n",
            "Epoch 35/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6934 - acc: 0.4982Epoch 00035: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 405us/step - loss: 0.6934 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5040\n",
            "Epoch 36/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5006Epoch 00036: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 429us/step - loss: 0.6932 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5071\n",
            "Epoch 37/150\n",
            "8176/8224 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.5048Epoch 00037: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 399us/step - loss: 0.6933 - acc: 0.5045 - val_loss: 0.6930 - val_acc: 0.5031\n",
            "Epoch 38/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.5025Epoch 00038: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 395us/step - loss: 0.6933 - acc: 0.5017 - val_loss: 0.6931 - val_acc: 0.5040\n",
            "Epoch 39/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.4993Epoch 00039: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 408us/step - loss: 0.6933 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.5018\n",
            "Epoch 40/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5011Epoch 00040: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 410us/step - loss: 0.6932 - acc: 0.5011 - val_loss: 0.6930 - val_acc: 0.5058\n",
            "Epoch 41/150\n",
            "8080/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5099Epoch 00041: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 395us/step - loss: 0.6931 - acc: 0.5097 - val_loss: 0.6931 - val_acc: 0.5009\n",
            "Epoch 42/150\n",
            "8160/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5038Epoch 00042: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 406us/step - loss: 0.6931 - acc: 0.5045 - val_loss: 0.6933 - val_acc: 0.4893\n",
            "Epoch 43/150\n",
            "8208/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5099Epoch 00043: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 375us/step - loss: 0.6932 - acc: 0.5098 - val_loss: 0.6930 - val_acc: 0.4996\n",
            "Epoch 44/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.5102Epoch 00044: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 425us/step - loss: 0.6933 - acc: 0.5101 - val_loss: 0.6932 - val_acc: 0.5013\n",
            "Epoch 45/150\n",
            "8080/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5103Epoch 00045: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 415us/step - loss: 0.6931 - acc: 0.5105 - val_loss: 0.6931 - val_acc: 0.5018\n",
            "Epoch 46/150\n",
            "8096/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4964Epoch 00046: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 409us/step - loss: 0.6931 - acc: 0.4966 - val_loss: 0.6931 - val_acc: 0.5013\n",
            "Epoch 47/150\n",
            "8176/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5007Epoch 00047: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 436us/step - loss: 0.6931 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5040\n",
            "Epoch 48/150\n",
            "8176/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5040Epoch 00048: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 417us/step - loss: 0.6932 - acc: 0.5036 - val_loss: 0.6930 - val_acc: 0.5062\n",
            "Epoch 49/150\n",
            "8128/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5001Epoch 00049: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 430us/step - loss: 0.6932 - acc: 0.5010 - val_loss: 0.6930 - val_acc: 0.5223\n",
            "Epoch 50/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5001Epoch 00050: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 429us/step - loss: 0.6932 - acc: 0.5009 - val_loss: 0.6931 - val_acc: 0.5036\n",
            "Epoch 51/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5055Epoch 00051: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 406us/step - loss: 0.6932 - acc: 0.5044 - val_loss: 0.6930 - val_acc: 0.5469\n",
            "Epoch 52/150\n",
            "8160/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4972Epoch 00052: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 397us/step - loss: 0.6931 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.5085\n",
            "Epoch 53/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5061Epoch 00053: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 430us/step - loss: 0.6931 - acc: 0.5058 - val_loss: 0.6931 - val_acc: 0.5031\n",
            "Epoch 54/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.4989Epoch 00054: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 429us/step - loss: 0.6932 - acc: 0.4984 - val_loss: 0.6928 - val_acc: 0.5344\n",
            "Epoch 55/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.5036Epoch 00055: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 431us/step - loss: 0.6930 - acc: 0.5044 - val_loss: 0.6928 - val_acc: 0.5085\n",
            "Epoch 56/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5123Epoch 00056: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 424us/step - loss: 0.6930 - acc: 0.5124 - val_loss: 0.6929 - val_acc: 0.5000\n",
            "Epoch 57/150\n",
            "8128/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5065Epoch 00057: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 412us/step - loss: 0.6932 - acc: 0.5072 - val_loss: 0.6932 - val_acc: 0.5031\n",
            "Epoch 58/150\n",
            "8128/8224 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.4998Epoch 00058: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 404us/step - loss: 0.6933 - acc: 0.5007 - val_loss: 0.6931 - val_acc: 0.5058\n",
            "Epoch 59/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5084Epoch 00059: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 420us/step - loss: 0.6932 - acc: 0.5088 - val_loss: 0.6928 - val_acc: 0.5165\n",
            "Epoch 60/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6929 - acc: 0.5088Epoch 00060: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 409us/step - loss: 0.6929 - acc: 0.5089 - val_loss: 0.6929 - val_acc: 0.5058\n",
            "Epoch 61/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5027Epoch 00061: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 411us/step - loss: 0.6931 - acc: 0.5028 - val_loss: 0.6930 - val_acc: 0.5022\n",
            "Epoch 62/150\n",
            "8080/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5054Epoch 00062: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 398us/step - loss: 0.6932 - acc: 0.5041 - val_loss: 0.6930 - val_acc: 0.5116\n",
            "Epoch 63/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.4976Epoch 00063: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 409us/step - loss: 0.6932 - acc: 0.4976 - val_loss: 0.6930 - val_acc: 0.5223\n",
            "Epoch 64/150\n",
            "8080/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5046Epoch 00064: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 423us/step - loss: 0.6932 - acc: 0.5055 - val_loss: 0.6929 - val_acc: 0.5112\n",
            "Epoch 65/150\n",
            "8176/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5073Epoch 00065: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 417us/step - loss: 0.6932 - acc: 0.5072 - val_loss: 0.6930 - val_acc: 0.4996\n",
            "Epoch 66/150\n",
            "8160/8224 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.5070Epoch 00066: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 416us/step - loss: 0.6933 - acc: 0.5075 - val_loss: 0.6931 - val_acc: 0.5022\n",
            "Epoch 67/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5017Epoch 00067: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 414us/step - loss: 0.6931 - acc: 0.5021 - val_loss: 0.6931 - val_acc: 0.5027\n",
            "Epoch 68/150\n",
            "8128/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5009Epoch 00068: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 419us/step - loss: 0.6932 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.5058\n",
            "Epoch 69/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6929 - acc: 0.4980Epoch 00069: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 387us/step - loss: 0.6929 - acc: 0.4976 - val_loss: 0.6928 - val_acc: 0.5000\n",
            "Epoch 70/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.5031Epoch 00070: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 424us/step - loss: 0.6930 - acc: 0.5035 - val_loss: 0.6931 - val_acc: 0.5049\n",
            "Epoch 71/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4940Epoch 00071: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 420us/step - loss: 0.6931 - acc: 0.4943 - val_loss: 0.6929 - val_acc: 0.5232\n",
            "Epoch 72/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5015Epoch 00072: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 429us/step - loss: 0.6931 - acc: 0.5015 - val_loss: 0.6930 - val_acc: 0.5045\n",
            "Epoch 73/150\n",
            "8080/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5095Epoch 00073: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 423us/step - loss: 0.6932 - acc: 0.5086 - val_loss: 0.6931 - val_acc: 0.5036\n",
            "Epoch 74/150\n",
            "8128/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5020Epoch 00074: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 396us/step - loss: 0.6931 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.5058\n",
            "Epoch 75/150\n",
            "8128/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4996Epoch 00075: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 431us/step - loss: 0.6931 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5040\n",
            "Epoch 76/150\n",
            "8064/8224 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.5036Epoch 00076: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 422us/step - loss: 0.6930 - acc: 0.5038 - val_loss: 0.6931 - val_acc: 0.5000\n",
            "Epoch 77/150\n",
            "8208/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4994Epoch 00077: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 411us/step - loss: 0.6931 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.5040\n",
            "Epoch 78/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.4920Epoch 00078: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 403us/step - loss: 0.6933 - acc: 0.4933 - val_loss: 0.6931 - val_acc: 0.5067\n",
            "Epoch 79/150\n",
            "8080/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4962Epoch 00079: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 426us/step - loss: 0.6932 - acc: 0.4960 - val_loss: 0.6931 - val_acc: 0.5067\n",
            "Epoch 80/150\n",
            "8096/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5019Epoch 00080: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 385us/step - loss: 0.6931 - acc: 0.5015 - val_loss: 0.6931 - val_acc: 0.5062\n",
            "Epoch 81/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4941Epoch 00081: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 423us/step - loss: 0.6931 - acc: 0.4934 - val_loss: 0.6931 - val_acc: 0.5058\n",
            "Epoch 82/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.4962Epoch 00082: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 421us/step - loss: 0.6932 - acc: 0.4961 - val_loss: 0.6930 - val_acc: 0.5116\n",
            "Epoch 83/150\n",
            "8096/8224 [============================>.] - ETA: 0s - loss: 0.6925 - acc: 0.5136Epoch 00083: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 409us/step - loss: 0.6925 - acc: 0.5136 - val_loss: 0.6927 - val_acc: 0.5116\n",
            "Epoch 84/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.5063Epoch 00084: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 421us/step - loss: 0.6930 - acc: 0.5060 - val_loss: 0.6929 - val_acc: 0.5027\n",
            "Epoch 85/150\n",
            "8176/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5050Epoch 00085: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 421us/step - loss: 0.6932 - acc: 0.5051 - val_loss: 0.6930 - val_acc: 0.5058\n",
            "Epoch 86/150\n",
            "8176/8224 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.5105Epoch 00086: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 410us/step - loss: 0.6933 - acc: 0.5102 - val_loss: 0.6932 - val_acc: 0.5018\n",
            "Epoch 87/150\n",
            "8208/8224 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.5067Epoch 00087: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 424us/step - loss: 0.6932 - acc: 0.5067 - val_loss: 0.6930 - val_acc: 0.5058\n",
            "Epoch 88/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5063Epoch 00088: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 427us/step - loss: 0.6931 - acc: 0.5066 - val_loss: 0.6931 - val_acc: 0.5040\n",
            "Epoch 89/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6928 - acc: 0.5104Epoch 00089: val_loss improved from 0.69257 to 0.69255, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 3s 424us/step - loss: 0.6928 - acc: 0.5101 - val_loss: 0.6926 - val_acc: 0.5170\n",
            "Epoch 90/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.5037Epoch 00090: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 418us/step - loss: 0.6930 - acc: 0.5039 - val_loss: 0.6931 - val_acc: 0.5031\n",
            "Epoch 91/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.5074Epoch 00091: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 416us/step - loss: 0.6930 - acc: 0.5073 - val_loss: 0.6929 - val_acc: 0.5254\n",
            "Epoch 92/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5104Epoch 00092: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 419us/step - loss: 0.6931 - acc: 0.5103 - val_loss: 0.6931 - val_acc: 0.5054\n",
            "Epoch 93/150\n",
            "8096/8224 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.5099Epoch 00093: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 414us/step - loss: 0.6927 - acc: 0.5107 - val_loss: 0.6926 - val_acc: 0.5174\n",
            "Epoch 94/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.5098Epoch 00094: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 415us/step - loss: 0.6930 - acc: 0.5103 - val_loss: 0.6928 - val_acc: 0.5125\n",
            "Epoch 95/150\n",
            "8096/8224 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.5107Epoch 00095: val_loss improved from 0.69255 to 0.69228, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 3s 423us/step - loss: 0.6926 - acc: 0.5113 - val_loss: 0.6923 - val_acc: 0.5402\n",
            "Epoch 96/150\n",
            "8096/8224 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.5143Epoch 00096: val_loss improved from 0.69228 to 0.69203, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 3s 422us/step - loss: 0.6927 - acc: 0.5137 - val_loss: 0.6920 - val_acc: 0.5513\n",
            "Epoch 97/150\n",
            "8048/8224 [============================>.] - ETA: 0s - loss: 0.6923 - acc: 0.5152Epoch 00097: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 395us/step - loss: 0.6924 - acc: 0.5146 - val_loss: 0.6925 - val_acc: 0.5147\n",
            "Epoch 98/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6926 - acc: 0.5165Epoch 00098: val_loss improved from 0.69203 to 0.69172, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 3s 405us/step - loss: 0.6925 - acc: 0.5168 - val_loss: 0.6917 - val_acc: 0.5496\n",
            "Epoch 99/150\n",
            "8128/8224 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.5048Epoch 00099: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 417us/step - loss: 0.6930 - acc: 0.5056 - val_loss: 0.6929 - val_acc: 0.5174\n",
            "Epoch 100/150\n",
            "8096/8224 [============================>.] - ETA: 0s - loss: 0.6930 - acc: 0.5068Epoch 00100: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 408us/step - loss: 0.6929 - acc: 0.5071 - val_loss: 0.6921 - val_acc: 0.5286\n",
            "Epoch 101/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.5098Epoch 00101: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 425us/step - loss: 0.6927 - acc: 0.5102 - val_loss: 0.6925 - val_acc: 0.5254\n",
            "Epoch 102/150\n",
            "8208/8224 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.5097Epoch 00102: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 436us/step - loss: 0.6927 - acc: 0.5096 - val_loss: 0.6927 - val_acc: 0.5295\n",
            "Epoch 103/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.5114Epoch 00103: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 435us/step - loss: 0.6931 - acc: 0.5113 - val_loss: 0.6926 - val_acc: 0.5116\n",
            "Epoch 104/150\n",
            "8208/8224 [============================>.] - ETA: 0s - loss: 0.6926 - acc: 0.5085Epoch 00104: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 426us/step - loss: 0.6926 - acc: 0.5085 - val_loss: 0.6930 - val_acc: 0.5049\n",
            "Epoch 105/150\n",
            "8128/8224 [============================>.] - ETA: 0s - loss: 0.6926 - acc: 0.5103Epoch 00105: val_loss improved from 0.69172 to 0.69156, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 3s 411us/step - loss: 0.6926 - acc: 0.5105 - val_loss: 0.6916 - val_acc: 0.5429\n",
            "Epoch 106/150\n",
            "8096/8224 [============================>.] - ETA: 0s - loss: 0.6925 - acc: 0.5114Epoch 00106: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 405us/step - loss: 0.6925 - acc: 0.5108 - val_loss: 0.6925 - val_acc: 0.5165\n",
            "Epoch 107/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6928 - acc: 0.5096Epoch 00107: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 416us/step - loss: 0.6929 - acc: 0.5090 - val_loss: 0.6917 - val_acc: 0.5469\n",
            "Epoch 108/150\n",
            "8160/8224 [============================>.] - ETA: 0s - loss: 0.6928 - acc: 0.5091Epoch 00108: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 407us/step - loss: 0.6928 - acc: 0.5090 - val_loss: 0.6927 - val_acc: 0.5058\n",
            "Epoch 109/150\n",
            "8096/8224 [============================>.] - ETA: 0s - loss: 0.6926 - acc: 0.5119Epoch 00109: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 414us/step - loss: 0.6926 - acc: 0.5119 - val_loss: 0.6923 - val_acc: 0.5156\n",
            "Epoch 110/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6918 - acc: 0.5254Epoch 00110: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 395us/step - loss: 0.6919 - acc: 0.5254 - val_loss: 0.6923 - val_acc: 0.5161\n",
            "Epoch 111/150\n",
            "8064/8224 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.5123Epoch 00111: val_loss improved from 0.69156 to 0.69085, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 3s 382us/step - loss: 0.6928 - acc: 0.5124 - val_loss: 0.6908 - val_acc: 0.5540\n",
            "Epoch 112/150\n",
            "8208/8224 [============================>.] - ETA: 0s - loss: 0.6923 - acc: 0.5158Epoch 00112: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 400us/step - loss: 0.6924 - acc: 0.5157 - val_loss: 0.6912 - val_acc: 0.5437\n",
            "Epoch 113/150\n",
            "8176/8224 [============================>.] - ETA: 0s - loss: 0.6919 - acc: 0.5243Epoch 00113: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 408us/step - loss: 0.6919 - acc: 0.5243 - val_loss: 0.6916 - val_acc: 0.5246\n",
            "Epoch 114/150\n",
            "8080/8224 [============================>.] - ETA: 0s - loss: 0.6919 - acc: 0.5162Epoch 00114: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 419us/step - loss: 0.6918 - acc: 0.5178 - val_loss: 0.6914 - val_acc: 0.5415\n",
            "Epoch 115/150\n",
            "8128/8224 [============================>.] - ETA: 0s - loss: 0.6920 - acc: 0.5226Epoch 00115: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 415us/step - loss: 0.6920 - acc: 0.5215 - val_loss: 0.6932 - val_acc: 0.5049\n",
            "Epoch 116/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6924 - acc: 0.5136Epoch 00116: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 422us/step - loss: 0.6924 - acc: 0.5130 - val_loss: 0.6927 - val_acc: 0.5085\n",
            "Epoch 117/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6920 - acc: 0.5245Epoch 00117: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 402us/step - loss: 0.6920 - acc: 0.5243 - val_loss: 0.6918 - val_acc: 0.5192\n",
            "Epoch 118/150\n",
            "8096/8224 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.5151Epoch 00118: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 427us/step - loss: 0.6928 - acc: 0.5148 - val_loss: 0.6925 - val_acc: 0.5125\n",
            "Epoch 119/150\n",
            "8176/8224 [============================>.] - ETA: 0s - loss: 0.6922 - acc: 0.5176Epoch 00119: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 374us/step - loss: 0.6922 - acc: 0.5178 - val_loss: 0.6920 - val_acc: 0.5174\n",
            "Epoch 120/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6921 - acc: 0.5172Epoch 00120: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 414us/step - loss: 0.6921 - acc: 0.5167 - val_loss: 0.6912 - val_acc: 0.5348\n",
            "Epoch 121/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6928 - acc: 0.5132Epoch 00121: val_loss improved from 0.69085 to 0.69044, saving model to top-weights.hdf5\n",
            "8224/8224 [==============================] - 3s 407us/step - loss: 0.6927 - acc: 0.5136 - val_loss: 0.6904 - val_acc: 0.5312\n",
            "Epoch 122/150\n",
            "8080/8224 [============================>.] - ETA: 0s - loss: 0.6919 - acc: 0.5196Epoch 00122: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 401us/step - loss: 0.6919 - acc: 0.5199 - val_loss: 0.6912 - val_acc: 0.5188\n",
            "Epoch 123/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6924 - acc: 0.5107Epoch 00123: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 414us/step - loss: 0.6924 - acc: 0.5108 - val_loss: 0.6923 - val_acc: 0.5179\n",
            "Epoch 124/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6929 - acc: 0.5099Epoch 00124: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 394us/step - loss: 0.6929 - acc: 0.5101 - val_loss: 0.6930 - val_acc: 0.5040\n",
            "Epoch 125/150\n",
            "8208/8224 [============================>.] - ETA: 0s - loss: 0.6925 - acc: 0.5134Epoch 00125: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 425us/step - loss: 0.6925 - acc: 0.5133 - val_loss: 0.6928 - val_acc: 0.5080\n",
            "Epoch 126/150\n",
            "8176/8224 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.5113Epoch 00126: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 399us/step - loss: 0.6927 - acc: 0.5116 - val_loss: 0.6916 - val_acc: 0.5022\n",
            "Epoch 127/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6927 - acc: 0.5093Epoch 00127: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 422us/step - loss: 0.6927 - acc: 0.5100 - val_loss: 0.6911 - val_acc: 0.5022\n",
            "Epoch 128/150\n",
            "8096/8224 [============================>.] - ETA: 0s - loss: 0.6923 - acc: 0.5201Epoch 00128: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 416us/step - loss: 0.6923 - acc: 0.5198 - val_loss: 0.6909 - val_acc: 0.5616\n",
            "Epoch 129/150\n",
            "8176/8224 [============================>.] - ETA: 0s - loss: 0.6922 - acc: 0.5224Epoch 00129: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 371us/step - loss: 0.6922 - acc: 0.5227 - val_loss: 0.6906 - val_acc: 0.5478\n",
            "Epoch 130/150\n",
            "8064/8224 [============================>.] - ETA: 0s - loss: 0.6922 - acc: 0.5129Epoch 00130: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 405us/step - loss: 0.6922 - acc: 0.5134 - val_loss: 0.6911 - val_acc: 0.5527\n",
            "Epoch 131/150\n",
            "8160/8224 [============================>.] - ETA: 0s - loss: 0.6923 - acc: 0.5066Epoch 00131: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 416us/step - loss: 0.6922 - acc: 0.5074 - val_loss: 0.6915 - val_acc: 0.5308\n",
            "Epoch 132/150\n",
            "8128/8224 [============================>.] - ETA: 0s - loss: 0.6924 - acc: 0.5170Epoch 00132: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 413us/step - loss: 0.6924 - acc: 0.5175 - val_loss: 0.6913 - val_acc: 0.5103\n",
            "Epoch 133/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6933 - acc: 0.5053Epoch 00133: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 417us/step - loss: 0.6933 - acc: 0.5060 - val_loss: 0.6922 - val_acc: 0.5174\n",
            "Epoch 134/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6928 - acc: 0.5168Epoch 00134: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 383us/step - loss: 0.6928 - acc: 0.5174 - val_loss: 0.6919 - val_acc: 0.5330\n",
            "Epoch 135/150\n",
            "8160/8224 [============================>.] - ETA: 0s - loss: 0.6916 - acc: 0.5276Epoch 00135: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 415us/step - loss: 0.6915 - acc: 0.5278 - val_loss: 0.6906 - val_acc: 0.5487\n",
            "Epoch 136/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6915 - acc: 0.5228Epoch 00136: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 394us/step - loss: 0.6915 - acc: 0.5230 - val_loss: 0.6918 - val_acc: 0.5250\n",
            "Epoch 137/150\n",
            "8096/8224 [============================>.] - ETA: 0s - loss: 0.6926 - acc: 0.5146Epoch 00137: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 412us/step - loss: 0.6926 - acc: 0.5136 - val_loss: 0.6930 - val_acc: 0.5049\n",
            "Epoch 138/150\n",
            "8080/8224 [============================>.] - ETA: 0s - loss: 0.6929 - acc: 0.5067Epoch 00138: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 410us/step - loss: 0.6929 - acc: 0.5063 - val_loss: 0.6919 - val_acc: 0.5304\n",
            "Epoch 139/150\n",
            "8208/8224 [============================>.] - ETA: 0s - loss: 0.6913 - acc: 0.5219Epoch 00139: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 417us/step - loss: 0.6913 - acc: 0.5216 - val_loss: 0.6925 - val_acc: 0.5152\n",
            "Epoch 140/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6915 - acc: 0.5264Epoch 00140: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 393us/step - loss: 0.6915 - acc: 0.5261 - val_loss: 0.6915 - val_acc: 0.5290\n",
            "Epoch 141/150\n",
            "8096/8224 [============================>.] - ETA: 0s - loss: 0.6925 - acc: 0.5133Epoch 00141: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 417us/step - loss: 0.6925 - acc: 0.5133 - val_loss: 0.6909 - val_acc: 0.5571\n",
            "Epoch 142/150\n",
            "8144/8224 [============================>.] - ETA: 0s - loss: 0.6922 - acc: 0.5201Epoch 00142: val_loss did not improve\n",
            "8224/8224 [==============================] - 4s 431us/step - loss: 0.6922 - acc: 0.5195 - val_loss: 0.6929 - val_acc: 0.5094\n",
            "Epoch 143/150\n",
            "8128/8224 [============================>.] - ETA: 0s - loss: 0.6929 - acc: 0.5098Epoch 00143: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 411us/step - loss: 0.6929 - acc: 0.5111 - val_loss: 0.6914 - val_acc: 0.5393\n",
            "Epoch 144/150\n",
            "8208/8224 [============================>.] - ETA: 0s - loss: 0.6920 - acc: 0.5222Epoch 00144: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 414us/step - loss: 0.6921 - acc: 0.5218 - val_loss: 0.6909 - val_acc: 0.5429\n",
            "Epoch 145/150\n",
            "8128/8224 [============================>.] - ETA: 0s - loss: 0.6920 - acc: 0.5171Epoch 00145: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 410us/step - loss: 0.6920 - acc: 0.5175 - val_loss: 0.6926 - val_acc: 0.5094\n",
            "Epoch 146/150\n",
            "8112/8224 [============================>.] - ETA: 0s - loss: 0.6918 - acc: 0.5198Epoch 00146: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 404us/step - loss: 0.6918 - acc: 0.5203 - val_loss: 0.6912 - val_acc: 0.5362\n",
            "Epoch 147/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6917 - acc: 0.5204Epoch 00147: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 373us/step - loss: 0.6917 - acc: 0.5204 - val_loss: 0.6909 - val_acc: 0.5420\n",
            "Epoch 148/150\n",
            "8160/8224 [============================>.] - ETA: 0s - loss: 0.6914 - acc: 0.5188Epoch 00148: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 394us/step - loss: 0.6914 - acc: 0.5184 - val_loss: 0.6911 - val_acc: 0.5353\n",
            "Epoch 149/150\n",
            "8208/8224 [============================>.] - ETA: 0s - loss: 0.6912 - acc: 0.5261Epoch 00149: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 402us/step - loss: 0.6912 - acc: 0.5263 - val_loss: 0.6905 - val_acc: 0.5607\n",
            "Epoch 150/150\n",
            "8192/8224 [============================>.] - ETA: 0s - loss: 0.6925 - acc: 0.5184Epoch 00150: val_loss did not improve\n",
            "8224/8224 [==============================] - 3s 406us/step - loss: 0.6925 - acc: 0.5186 - val_loss: 0.6920 - val_acc: 0.5179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjooFFpr_0-y",
        "colab_type": "code",
        "outputId": "3bd6198c-40a4-4868-f259-21551799b395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYTBMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZKSAEBGRjBQQIiKS0f8HMhkC5rBi1nAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW9c3R6aOyl2",
        "colab_type": "text"
      },
      "source": [
        "**Объединяем сверточную и полносвязную сеть**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBNDmMY3l9bI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLBg8jAfz8xU",
        "colab_type": "code",
        "outputId": "7cbbf8dc-f5dc-4896-e3ff-8669fa61d158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.models import Model\n",
        "\n",
        "batch_size = 16\n",
        "epochs = 50\n",
        "\n",
        "input_tensor = Input(shape=(img_height,img_width,3))\n",
        "\n",
        "base_model = SqueezeNet(weights='imagenet',\n",
        "                          include_top=False,\n",
        "                          input_shape=(img_width, img_height, 3),\n",
        "                          pooling='avg')\n",
        "\n",
        "top_model = Sequential()\n",
        "    \n",
        "top_model.add(Dense(512, activation='relu', input_shape=base_model.output_shape[1:]))\n",
        "top_model.add(Dropout(0.5))\n",
        "\n",
        "top_model.add(Dense(256, activation='relu', input_shape=base_model.output_shape[1:]))\n",
        "top_model.add(Dropout(0.5))\n",
        "\n",
        "top_model.add(Dense(128, activation='relu', input_shape=base_model.output_shape[1:]))\n",
        "top_model.add(Dropout(0.5))  \n",
        "\n",
        "top_model.add(Dense(64, activation='relu'))\n",
        "top_model.add(Dropout(0.5))  \n",
        "\n",
        "top_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "top_model.load_weights('top-weights.hdf5')\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
        "\n",
        "model.compile(optimizer=SGD(lr=0.005, momentum=0.1, nesterov=True),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
        "\n",
        "train_datagen = image.ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "test_datagen = image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_height, img_width),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "\n",
        "history = model.fit_generator(train_generator,\n",
        "                            steps_per_epoch=nb_train_samples // batch_size,\n",
        "                            epochs=epochs,\n",
        "                            validation_data=validation_generator,\n",
        "                            validation_steps=nb_validation_samples // batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8224 images belonging to 2 classes.\n",
            "Found 2240 images belonging to 2 classes.\n",
            "Epoch 1/50\n",
            "514/514 [==============================] - 167s 325ms/step - loss: 0.6936 - acc: 0.5078 - val_loss: 0.6934 - val_acc: 0.5022\n",
            "Epoch 2/50\n",
            "514/514 [==============================] - 161s 314ms/step - loss: 0.6943 - acc: 0.5013 - val_loss: 0.6914 - val_acc: 0.5540\n",
            "Epoch 3/50\n",
            "514/514 [==============================] - 159s 309ms/step - loss: 0.6937 - acc: 0.5018 - val_loss: 0.6921 - val_acc: 0.5170\n",
            "Epoch 4/50\n",
            "514/514 [==============================] - 158s 307ms/step - loss: 0.6931 - acc: 0.5168 - val_loss: 0.6932 - val_acc: 0.5018\n",
            "Epoch 5/50\n",
            "514/514 [==============================] - 156s 304ms/step - loss: 0.6932 - acc: 0.5001 - val_loss: 0.6931 - val_acc: 0.5045\n",
            "Epoch 6/50\n",
            "514/514 [==============================] - 157s 305ms/step - loss: 0.6937 - acc: 0.5038 - val_loss: 0.6928 - val_acc: 0.5232\n",
            "Epoch 7/50\n",
            "514/514 [==============================] - 156s 303ms/step - loss: 0.6933 - acc: 0.5068 - val_loss: 0.6931 - val_acc: 0.5040\n",
            "Epoch 8/50\n",
            "514/514 [==============================] - 156s 304ms/step - loss: 0.6929 - acc: 0.5100 - val_loss: 0.6897 - val_acc: 0.5214\n",
            "Epoch 9/50\n",
            "514/514 [==============================] - 154s 300ms/step - loss: 0.6925 - acc: 0.5264 - val_loss: 0.6893 - val_acc: 0.5710\n",
            "Epoch 10/50\n",
            "514/514 [==============================] - 156s 303ms/step - loss: 0.6919 - acc: 0.5175 - val_loss: 0.6920 - val_acc: 0.5089\n",
            "Epoch 11/50\n",
            "514/514 [==============================] - 153s 298ms/step - loss: 0.6932 - acc: 0.5156 - val_loss: 0.6931 - val_acc: 0.5036\n",
            "Epoch 12/50\n",
            "514/514 [==============================] - 155s 301ms/step - loss: 0.6942 - acc: 0.5040 - val_loss: 0.6911 - val_acc: 0.5397\n",
            "Epoch 13/50\n",
            "514/514 [==============================] - 154s 299ms/step - loss: 0.6908 - acc: 0.5249 - val_loss: 0.6869 - val_acc: 0.5652\n",
            "Epoch 14/50\n",
            "514/514 [==============================] - 155s 301ms/step - loss: 0.6926 - acc: 0.5173 - val_loss: 0.6923 - val_acc: 0.5094\n",
            "Epoch 15/50\n",
            "514/514 [==============================] - 153s 297ms/step - loss: 0.6902 - acc: 0.5393 - val_loss: 0.6930 - val_acc: 0.5263\n",
            "Epoch 16/50\n",
            "514/514 [==============================] - 156s 303ms/step - loss: 0.6918 - acc: 0.5192 - val_loss: 0.6880 - val_acc: 0.5598\n",
            "Epoch 17/50\n",
            "514/514 [==============================] - 154s 300ms/step - loss: 0.6895 - acc: 0.5368 - val_loss: 0.6914 - val_acc: 0.5143\n",
            "Epoch 18/50\n",
            "514/514 [==============================] - 156s 303ms/step - loss: 0.6928 - acc: 0.5215 - val_loss: 0.6920 - val_acc: 0.5143\n",
            "Epoch 19/50\n",
            "514/514 [==============================] - 153s 298ms/step - loss: 0.6897 - acc: 0.5400 - val_loss: 0.6919 - val_acc: 0.5152\n",
            "Epoch 20/50\n",
            "514/514 [==============================] - 154s 299ms/step - loss: 0.6883 - acc: 0.5404 - val_loss: 0.6876 - val_acc: 0.5473\n",
            "Epoch 21/50\n",
            "514/514 [==============================] - 153s 299ms/step - loss: 0.6854 - acc: 0.5471 - val_loss: 0.6889 - val_acc: 0.5362\n",
            "Epoch 22/50\n",
            "514/514 [==============================] - 153s 298ms/step - loss: 0.6826 - acc: 0.5654 - val_loss: 0.6863 - val_acc: 0.5536\n",
            "Epoch 23/50\n",
            "514/514 [==============================] - 152s 295ms/step - loss: 0.6824 - acc: 0.5646 - val_loss: 0.6850 - val_acc: 0.5558\n",
            "Epoch 24/50\n",
            "514/514 [==============================] - 154s 299ms/step - loss: 0.6810 - acc: 0.5698 - val_loss: 0.6932 - val_acc: 0.5112\n",
            "Epoch 25/50\n",
            "514/514 [==============================] - 150s 293ms/step - loss: 0.6792 - acc: 0.5745 - val_loss: 0.6777 - val_acc: 0.5924\n",
            "Epoch 26/50\n",
            "514/514 [==============================] - 153s 298ms/step - loss: 0.6758 - acc: 0.5773 - val_loss: 0.6825 - val_acc: 0.5638\n",
            "Epoch 27/50\n",
            "514/514 [==============================] - 153s 297ms/step - loss: 0.6767 - acc: 0.5882 - val_loss: 0.6848 - val_acc: 0.5464\n",
            "Epoch 28/50\n",
            "514/514 [==============================] - 153s 298ms/step - loss: 0.6703 - acc: 0.5956 - val_loss: 0.6623 - val_acc: 0.6246\n",
            "Epoch 29/50\n",
            "514/514 [==============================] - 153s 297ms/step - loss: 0.6695 - acc: 0.5985 - val_loss: 0.6638 - val_acc: 0.6268\n",
            "Epoch 30/50\n",
            "514/514 [==============================] - 152s 295ms/step - loss: 0.6698 - acc: 0.5972 - val_loss: 0.6757 - val_acc: 0.5759\n",
            "Epoch 31/50\n",
            "514/514 [==============================] - 151s 293ms/step - loss: 0.6648 - acc: 0.6125 - val_loss: 0.6688 - val_acc: 0.5991\n",
            "Epoch 32/50\n",
            "514/514 [==============================] - 152s 296ms/step - loss: 0.6625 - acc: 0.6107 - val_loss: 0.6564 - val_acc: 0.6455\n",
            "Epoch 33/50\n",
            "514/514 [==============================] - 153s 298ms/step - loss: 0.6578 - acc: 0.6229 - val_loss: 0.6758 - val_acc: 0.5656\n",
            "Epoch 34/50\n",
            "514/514 [==============================] - 151s 294ms/step - loss: 0.6566 - acc: 0.6201 - val_loss: 0.6935 - val_acc: 0.5237\n",
            "Epoch 35/50\n",
            "514/514 [==============================] - 152s 295ms/step - loss: 0.6548 - acc: 0.6226 - val_loss: 0.6669 - val_acc: 0.5853\n",
            "Epoch 36/50\n",
            "514/514 [==============================] - 152s 295ms/step - loss: 0.6516 - acc: 0.6254 - val_loss: 0.6760 - val_acc: 0.5656\n",
            "Epoch 37/50\n",
            "514/514 [==============================] - 152s 296ms/step - loss: 0.6473 - acc: 0.6328 - val_loss: 0.6556 - val_acc: 0.6246\n",
            "Epoch 38/50\n",
            "514/514 [==============================] - 152s 296ms/step - loss: 0.6492 - acc: 0.6305 - val_loss: 0.6446 - val_acc: 0.6545\n",
            "Epoch 39/50\n",
            "514/514 [==============================] - 153s 297ms/step - loss: 0.6439 - acc: 0.6373 - val_loss: 0.6780 - val_acc: 0.5558\n",
            "Epoch 40/50\n",
            "514/514 [==============================] - 151s 293ms/step - loss: 0.6399 - acc: 0.6452 - val_loss: 0.7055 - val_acc: 0.5129\n",
            "Epoch 41/50\n",
            "514/514 [==============================] - 150s 293ms/step - loss: 0.6387 - acc: 0.6443 - val_loss: 0.6599 - val_acc: 0.5973\n",
            "Epoch 42/50\n",
            "514/514 [==============================] - 152s 296ms/step - loss: 0.6363 - acc: 0.6463 - val_loss: 0.6442 - val_acc: 0.6446\n",
            "Epoch 43/50\n",
            "514/514 [==============================] - 151s 294ms/step - loss: 0.6327 - acc: 0.6555 - val_loss: 0.6838 - val_acc: 0.5433\n",
            "Epoch 44/50\n",
            "514/514 [==============================] - 151s 294ms/step - loss: 0.6317 - acc: 0.6492 - val_loss: 0.6424 - val_acc: 0.6411\n",
            "Epoch 45/50\n",
            "514/514 [==============================] - 151s 294ms/step - loss: 0.6322 - acc: 0.6483 - val_loss: 0.6529 - val_acc: 0.6196\n",
            "Epoch 46/50\n",
            "514/514 [==============================] - 152s 295ms/step - loss: 0.6301 - acc: 0.6520 - val_loss: 0.6366 - val_acc: 0.6504\n",
            "Epoch 47/50\n",
            "514/514 [==============================] - 151s 293ms/step - loss: 0.6249 - acc: 0.6600 - val_loss: 0.6470 - val_acc: 0.6254\n",
            "Epoch 48/50\n",
            "514/514 [==============================] - 153s 297ms/step - loss: 0.6201 - acc: 0.6642 - val_loss: 0.6310 - val_acc: 0.6634\n",
            "Epoch 49/50\n",
            "514/514 [==============================] - 152s 296ms/step - loss: 0.6245 - acc: 0.6572 - val_loss: 0.6410 - val_acc: 0.6366\n",
            "Epoch 50/50\n",
            "514/514 [==============================] - 152s 295ms/step - loss: 0.6231 - acc: 0.6598 - val_loss: 0.6363 - val_acc: 0.6469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSfO5B7cfOUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiD9A9V_PFhl",
        "colab_type": "text"
      },
      "source": [
        "**Тестируем полученную сеть**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPkKI4WJmMFF",
        "colab_type": "code",
        "outputId": "3ddaf501-4bf0-4f82-f286-240ac25d6adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "test_generator = data_generator.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=False)\n",
        "\n",
        "score = model.evaluate_generator(test_generator, batch_size)\n",
        "\n",
        "print(\"Loss: \", score[0], \"Accuracy: \", score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2401 images belonging to 2 classes.\n",
            "Loss:  0.6386033137457314 Accuracy:  0.6438983756768013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbDD4tT2PeZK",
        "colab_type": "text"
      },
      "source": [
        "**Сохраняем лучшие веса сети**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZJRzFTu0Vef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('fake_orig_squeezenet_v1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQdQkuin497A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
